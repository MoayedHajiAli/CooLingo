{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/users/mali18/CooLingo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "while not os.getcwd().endswith('CooLingo'):\n",
    "    os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.liveportrait import LiveSpeechPortraits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt: Namespace(A2L_GMM_ndim=75, APC_frame_history=0, APC_hidden_size=512, APC_residual=False, APC_rnn_layers=3, FPS=60, LSTM_dropout=0, LSTM_hidden_size=256, LSTM_layers=3, LSTM_output_size=80, LSTM_residual=False, LSTM_sequence_length=60, audioRF_future=0, audioRF_history=60, audio_encoder='APC', batch_size=32, checkpoints_dir='./checkpoints/', dataroot='default_path', dataset_mode='audiovisual', dataset_names='default_name', eval=False, feature_decoder='LSTM', feature_dtype='pts3d', frame_future=18, frame_jump_stride=4, gpu_ids='0', ispts_norm=1, load_epoch='500', loss='L2', max_dataset_size=inf, model='audio2feature', name='Audio2Feature', num_threads=0, only_mouth=1, phase='test', predict_length=1, sample_rate=16000, sequence_length=240, serial_batches=False, suffix='', task='Audio2Feature', time_frame_length=1, use_delta_pts=1, verbose=False)\n",
      "----------------- Options ---------------\n",
      "             A2L_GMM_ndim: 75                            \n",
      "        APC_frame_history: 0                             \n",
      "          APC_hidden_size: 512                           \n",
      "             APC_residual: False                         \n",
      "           APC_rnn_layers: 3                             \n",
      "                      FPS: 60                            \n",
      "             LSTM_dropout: 0                             \n",
      "         LSTM_hidden_size: 256                           \n",
      "              LSTM_layers: 3                             \n",
      "         LSTM_output_size: 80                            \n",
      "            LSTM_residual: False                         \n",
      "     LSTM_sequence_length: 60                            \n",
      "           audioRF_future: 0                             \n",
      "          audioRF_history: 60                            \n",
      "            audio_encoder: APC                           \n",
      "               batch_size: 32                            \n",
      "          checkpoints_dir: ./checkpoints/                \n",
      "                 dataroot: default_path                  \n",
      "             dataset_mode: audiovisual                   \n",
      "            dataset_names: default_name                  \n",
      "                     eval: False                         \n",
      "          feature_decoder: LSTM                          \n",
      "            feature_dtype: pts3d                         \n",
      "             frame_future: 18                            \n",
      "        frame_jump_stride: 4                             \n",
      "                  gpu_ids: 0                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               ispts_norm: 1                             \n",
      "               load_epoch: 500                           \n",
      "                     loss: L2                            \n",
      "         max_dataset_size: inf                           \n",
      "                    model: audio2feature                 \n",
      "                     name: Audio2Feature                 \n",
      "              num_threads: 0                             \n",
      "               only_mouth: 1                             \n",
      "                    phase: test                          \n",
      "           predict_length: 1                             \n",
      "              sample_rate: 16000                         \n",
      "          sequence_length: 240                           \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                     task: Audio2Feature                 \n",
      "        time_frame_length: 1                             \n",
      "            use_delta_pts: 1                             \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "----------------- Options ---------------\n",
      "          A2H_GMM_ncenter: 1                             \n",
      "             A2H_GMM_ndim: 12                            \n",
      "        A2H_GMM_sigma_min: 0.03                          \n",
      "         A2H_wavenet_cond: True                          \n",
      "A2H_wavenet_cond_channels: 512                           \n",
      "A2H_wavenet_dilation_channels: 128                           \n",
      "A2H_wavenet_input_channels: 12                            \n",
      "  A2H_wavenet_kernel_size: 2                             \n",
      "A2H_wavenet_residual_blocks: 2                             \n",
      "A2H_wavenet_residual_channels: 128                           \n",
      "A2H_wavenet_residual_layers: 7                             \n",
      "A2H_wavenet_skip_channels: 256                           \n",
      "     A2H_wavenet_use_bias: True                          \n",
      "        APC_frame_history: 60                            \n",
      "          APC_hidden_size: 512                           \n",
      "             APC_residual: False                         \n",
      "           APC_rnn_layers: 3                             \n",
      "                      FPS: 60                            \n",
      "           audioRF_future: 0                             \n",
      "          audioRF_history: 60                            \n",
      "            audio_encoder: APC                           \n",
      "            audio_windows: 2                             \n",
      "audiofeature_input_channels: 80                            \n",
      "               batch_size: 32                            \n",
      "          checkpoints_dir: ./checkpoints/                \n",
      "                 dataroot: path                          \n",
      "             dataset_mode: audiovisual                   \n",
      "            dataset_names: name                          \n",
      "                     eval: False                         \n",
      "          feature_decoder: WaveNet                       \n",
      "             frame_future: 15                            \n",
      "        frame_jump_stride: 1                             \n",
      "                  gpu_ids: 0                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               load_epoch: 500                           \n",
      "                     loss: GMM                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: audio2headpose                \n",
      "                     name: Audio2Headpose                \n",
      "              num_threads: 0                             \n",
      "                    phase: test                          \n",
      "           predict_length: 5                             \n",
      "              sample_rate: 16000                         \n",
      "          sequence_length: 240                           \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                     task: Audio2Headpose                \n",
      "        time_frame_length: 1                             \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "------------ Options -------------\n",
      "batch_size: 1\n",
      "checkpoints_dir: ./checkpoints/\n",
      "dataroot: ./data/\n",
      "dataset_mode: face\n",
      "dataset_names: ['name']\n",
      "debug: False\n",
      "display_id: 0\n",
      "display_winsize: 512\n",
      "fineSize: 512\n",
      "fp16: 0\n",
      "gpu_ids: [0]\n",
      "input_nc: 1\n",
      "isH5: 1\n",
      "isMask: 0\n",
      "isTrain: False\n",
      "loadSize: 512\n",
      "load_pretrain: \n",
      "local_rank: 0\n",
      "max_dataset_size: inf\n",
      "model: feature2face\n",
      "n_blocks_E: 3\n",
      "n_downsample_E: 3\n",
      "n_downsample_G: 8\n",
      "name: TestRender\n",
      "ngf: 64\n",
      "ngf_E: 16\n",
      "no_flip: 1\n",
      "num_threads: 0\n",
      "output_nc: 3\n",
      "phase: test\n",
      "resize_or_crop: scaleWidth\n",
      "serial_batches: False\n",
      "suffix: .jpg\n",
      "task: Feature2Face\n",
      "test_dataset_names: ['name']\n",
      "tf_log: True\n",
      "verbose: False\n",
      "-------------- End ----------------\n",
      "---------- Loading Model: APC-------------\n",
      "---------- Loading Model: Audio2Feature -------------\n",
      "initialize network with normal\n",
      "model [Audio2FeatureModel] was created\n",
      "loading the model from pretrained_models/LiveSpeechPortraits/data/May/checkpoints/Audio2Feature.pkl\n",
      "---------- Networks initialized -------------\n",
      "[Network Audio2Feature] Total number of parameters : 3.064 M\n",
      "-----------------------------------------------\n",
      "---------- Loading Model: Audio2Headpose -------------\n",
      "initialize network with normal\n",
      "model [Audio2HeadposeModel] was created\n",
      "loading the model from pretrained_models/LiveSpeechPortraits/data/May/checkpoints/Audio2Headpose.pkl\n",
      "---------- Networks initialized -------------\n",
      "[Network Audio2Headpose] Total number of parameters : 4.267 M\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'WaveNet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_141088/2089107406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiveSpeechPortraits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/users/mali18/CooLingo/src/liveportrait.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, id, apc_model_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHeadopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA2H_receptive_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio2Headpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio2Headpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWaveNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceptive_field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHeadopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA2H_receptive_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio2Headpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio2Headpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWaveNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceptive_field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------- Loading Model: {} -------------'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRenderopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/taming-default/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'WaveNet'"
     ]
    }
   ],
   "source": [
    "model = LiveSpeechPortraits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /scratch/users/mali18/.conda/envs/taming-default/lib/python3.8/site-packages (6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists('pretrained_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/users/mali18/CooLingo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coolingo)",
   "language": "python",
   "name": "coolingo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
