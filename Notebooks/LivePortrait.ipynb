{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "while not os.getcwd().endswith('CooLingo'):\n",
    "    os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dominate in /Users/moayedhajiali/anaconda3/envs/coolingo/lib/python3.8/site-packages (2.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dominate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.liveportrait import LiveSpeechPortraits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt: Namespace(A2L_GMM_ndim=75, APC_frame_history=0, APC_hidden_size=512, APC_residual=False, APC_rnn_layers=3, FPS=60, LSTM_dropout=0, LSTM_hidden_size=256, LSTM_layers=3, LSTM_output_size=80, LSTM_residual=False, LSTM_sequence_length=60, audioRF_future=0, audioRF_history=60, audio_encoder='APC', batch_size=32, checkpoints_dir='./checkpoints/', dataroot='default_path', dataset_mode='audiovisual', dataset_names='default_name', eval=False, feature_decoder='LSTM', feature_dtype='pts3d', frame_future=18, frame_jump_stride=4, gpu_ids='0', ispts_norm=1, load_epoch='500', loss='L2', max_dataset_size=inf, model='audio2feature', name='Audio2Feature', num_threads=0, only_mouth=1, phase='test', predict_length=1, sample_rate=16000, sequence_length=240, serial_batches=False, suffix='', task='Audio2Feature', time_frame_length=1, use_delta_pts=1, verbose=False)\n",
      "----------------- Options ---------------\n",
      "             A2L_GMM_ndim: 75                            \n",
      "        APC_frame_history: 0                             \n",
      "          APC_hidden_size: 512                           \n",
      "             APC_residual: False                         \n",
      "           APC_rnn_layers: 3                             \n",
      "                      FPS: 60                            \n",
      "             LSTM_dropout: 0                             \n",
      "         LSTM_hidden_size: 256                           \n",
      "              LSTM_layers: 3                             \n",
      "         LSTM_output_size: 80                            \n",
      "            LSTM_residual: False                         \n",
      "     LSTM_sequence_length: 60                            \n",
      "           audioRF_future: 0                             \n",
      "          audioRF_history: 60                            \n",
      "            audio_encoder: APC                           \n",
      "               batch_size: 32                            \n",
      "          checkpoints_dir: ./checkpoints/                \n",
      "                 dataroot: default_path                  \n",
      "             dataset_mode: audiovisual                   \n",
      "            dataset_names: default_name                  \n",
      "                     eval: False                         \n",
      "          feature_decoder: LSTM                          \n",
      "            feature_dtype: pts3d                         \n",
      "             frame_future: 18                            \n",
      "        frame_jump_stride: 4                             \n",
      "                  gpu_ids: 0                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               ispts_norm: 1                             \n",
      "               load_epoch: 500                           \n",
      "                     loss: L2                            \n",
      "         max_dataset_size: inf                           \n",
      "                    model: audio2feature                 \n",
      "                     name: Audio2Feature                 \n",
      "              num_threads: 0                             \n",
      "               only_mouth: 1                             \n",
      "                    phase: test                          \n",
      "           predict_length: 1                             \n",
      "              sample_rate: 16000                         \n",
      "          sequence_length: 240                           \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                     task: Audio2Feature                 \n",
      "        time_frame_length: 1                             \n",
      "            use_delta_pts: 1                             \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "----------------- Options ---------------\n",
      "          A2H_GMM_ncenter: 1                             \n",
      "             A2H_GMM_ndim: 12                            \n",
      "        A2H_GMM_sigma_min: 0.03                          \n",
      "         A2H_wavenet_cond: True                          \n",
      "A2H_wavenet_cond_channels: 512                           \n",
      "A2H_wavenet_dilation_channels: 128                           \n",
      "A2H_wavenet_input_channels: 12                            \n",
      "  A2H_wavenet_kernel_size: 2                             \n",
      "A2H_wavenet_residual_blocks: 2                             \n",
      "A2H_wavenet_residual_channels: 128                           \n",
      "A2H_wavenet_residual_layers: 7                             \n",
      "A2H_wavenet_skip_channels: 256                           \n",
      "     A2H_wavenet_use_bias: True                          \n",
      "        APC_frame_history: 60                            \n",
      "          APC_hidden_size: 512                           \n",
      "             APC_residual: False                         \n",
      "           APC_rnn_layers: 3                             \n",
      "                      FPS: 60                            \n",
      "           audioRF_future: 0                             \n",
      "          audioRF_history: 60                            \n",
      "            audio_encoder: APC                           \n",
      "            audio_windows: 2                             \n",
      "audiofeature_input_channels: 80                            \n",
      "               batch_size: 32                            \n",
      "          checkpoints_dir: ./checkpoints/                \n",
      "                 dataroot: path                          \n",
      "             dataset_mode: audiovisual                   \n",
      "            dataset_names: name                          \n",
      "                     eval: False                         \n",
      "          feature_decoder: WaveNet                       \n",
      "             frame_future: 15                            \n",
      "        frame_jump_stride: 1                             \n",
      "                  gpu_ids: 0                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               load_epoch: 500                           \n",
      "                     loss: GMM                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: audio2headpose                \n",
      "                     name: Audio2Headpose                \n",
      "              num_threads: 0                             \n",
      "                    phase: test                          \n",
      "           predict_length: 5                             \n",
      "              sample_rate: 16000                         \n",
      "          sequence_length: 240                           \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                     task: Audio2Headpose                \n",
      "        time_frame_length: 1                             \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "------------ Options -------------\n",
      "batch_size: 1\n",
      "checkpoints_dir: ./checkpoints/\n",
      "dataroot: ./data/\n",
      "dataset_mode: face\n",
      "dataset_names: ['name']\n",
      "debug: False\n",
      "display_id: 0\n",
      "display_winsize: 512\n",
      "fineSize: 512\n",
      "fp16: 0\n",
      "gpu_ids: [0]\n",
      "input_nc: 1\n",
      "isH5: 1\n",
      "isMask: 0\n",
      "isTrain: False\n",
      "loadSize: 512\n",
      "load_pretrain: \n",
      "local_rank: 0\n",
      "max_dataset_size: inf\n",
      "model: feature2face\n",
      "n_blocks_E: 3\n",
      "n_downsample_E: 3\n",
      "n_downsample_G: 8\n",
      "name: TestRender\n",
      "ngf: 64\n",
      "ngf_E: 16\n",
      "no_flip: 1\n",
      "num_threads: 0\n",
      "output_nc: 3\n",
      "phase: test\n",
      "resize_or_crop: scaleWidth\n",
      "serial_batches: False\n",
      "suffix: .jpg\n",
      "task: Feature2Face\n",
      "test_dataset_names: ['name']\n",
      "tf_log: True\n",
      "verbose: False\n",
      "-------------- End ----------------\n",
      "---------- Loading Model: APC-------------\n",
      "pretrained_models/LiveSpeechPortraits/data/APC_epoch_160.model\n",
      "cpu\n",
      "---------- Loading Model: Audio2Feature -------------\n",
      "initialize network with normal\n",
      "model [Audio2FeatureModel] was created\n",
      "loading the model from pretrained_models/LiveSpeechPortraits/data/May/checkpoints/Audio2Feature.pkl\n",
      "---------- Networks initialized -------------\n",
      "[Network Audio2Feature] Total number of parameters : 3.064 M\n",
      "-----------------------------------------------\n",
      "---------- Loading Model: Audio2Headpose -------------\n",
      "initialize network with normal\n",
      "model [Audio2HeadposeModel] was created\n",
      "loading the model from pretrained_models/LiveSpeechPortraits/data/May/checkpoints/Audio2Headpose.pkl\n",
      "---------- Networks initialized -------------\n",
      "[Network Audio2Headpose] Total number of parameters : 4.267 M\n",
      "-----------------------------------------------\n",
      "---------- Loading Model: Feature2Face -------------\n",
      "dataset [FaceDataset] was created\n",
      "Namespace(batch_size=1, checkpoints_dir='./checkpoints/', dataroot='pretrained_models/LiveSpeechPortraits/data/May/', dataset_mode='face', dataset_names=['name'], debug=False, display_id=0, display_winsize=512, fineSize=512, fp16=0, gpu_ids=[], input_nc=1, isH5=1, isMask=0, isTrain=False, loadSize=512, load_epoch='pretrained_models/LiveSpeechPortraits/data/May/checkpoints/Feature2Face.pkl', load_pretrain='', local_rank=0, max_dataset_size=inf, model='feature2face', n_blocks_E=3, n_downsample_E=3, n_downsample_G=8, name='TestRender', ngf=64, ngf_E=16, no_flip=1, num_threads=0, output_nc=3, phase='test', resize_or_crop='scaleWidth', serial_batches=False, size='large', suffix='.jpg', task='Feature2Face', test_dataset_names=['name'], tf_log=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generator networks initialized -------------\n",
      "-------------------------------------------------------\n",
      "initialize network with normal\n",
      "model [Feature2FaceModel] was created\n",
      "loading the model from pretrained_models/LiveSpeechPortraits/data/May/checkpoints/Feature2Face.pkl\n",
      "---------- Networks initialized -------------\n",
      "[Network Feature2Face_G] Total number of parameters : 121.790 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LiveSpeechPortraits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Computing APC features...\n",
      "2. Manifold projection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLE projection: 100%|███████████████████████████████████████████████████████████| 822/822 [00:00<00:00, 17111.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Audio2Mouth inference...\n",
      "4. Headpose inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating headpose: 100%|█████████████████████████████████████████████████████████| 396/396 [00:20<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Post-processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 396/396 [00:00<00:00, 22650.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Image2Image translation & Saving results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Image2Image translation inference:   0%|                                                      | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 512, 512]) torch.Size([32, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.generate_protrait('sample_data/audio/sample.wav', fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/users/mali18/CooLingo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.repeat(3, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
